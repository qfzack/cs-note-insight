# 分布式系统

## 基本概念

### 分布式系统是什么

分布式系统（Distributed System）是指由一组通过网络进行通信、为了完成共同任务而协调工作的计算机节点，并作为一个整体向用户提供统一服务的系统

### 分布式系统的优势

- 高性能与高吞吐：通过多台机器并行处理任务，处理能力远高于单台机器
- 高可用性：一台机器宕机了，其他的机器可以接管业务，系统整体不会瘫痪
- 可扩展性：业务量增加时，只需要往集群中添加机器（水平扩展）即可，而不需要更换更大更强的机器（垂直扩展）
- 低延迟：可以在离用户更近的节点部署服务，减少网络传输时间

### 分布式系统带来的问题

- 网络不可靠：网络延迟、丢包、分区等问题会影响系统的稳定性
- 数据一致性问题：多个节点上的数据拷贝如何保持同步
- 节点故障：随机机器数量的增加，总有机器会出现故障，如何保证系统的可用性
- 系统复杂度：分布式系统涉及多个节点的协作，部署和维护都比单机系统复杂得多

### 如何解决分布式系统的问题

- 数据一致性：使用共识算法（如Paxos、Raft）或分布式锁（如Zookeeper、etcd）
- 流量分发：使用负载均衡器（如Nginx、HAProxy）将请求分发到不同节点
- 容错处理：引入熔断、降级、限流机制，防止单点故障影响整体系统
- 服务发现：建立注册中心（如Consul、Nacos）动态管理节点信息
- 分布式事务：使用2PC（两阶段提交）、TCC（Try-Confirm-Cancel）等方式保证跨节点操作的一致性

### K8s在分布式系统中的作用

K8s通过平台层面的自动化调度和抽象，极大缓解了分布式系统在运维、协作和可靠性方面的问题

1. 节点故障和高可用问题
    - 自愈机制：K8s会持续监控容器状态，如果某个容器或节点出现故障，K8s会自动重新调度容器到健康的节点上，保证服务的持续可用
    - 健康检查：通过Liveness和Readiness探针，K8s可以及时发现不健康的服务实例并进行处理
2. 服务发现和负载均衡问题
    - Service抽象：分布式系统中，机器的IP地址和端口可能会动态变化，K8s的Service资源为一组Pod提供了一个稳定的访问入口，隐藏了底层实例的变化
    - 负载均衡：流量达到K8s的Service后，会通过kube-proxy将请求分发到后端的多个Pod实例，实现流量分发和负载均衡
    - DNS服务：K8s内置的DNS服务允许服务之间通过服务名进行访问，简化了服务间通信
3. 扩展性问题
    - 水平扩展HPA（Horizontal Pod Autoscaler）：K8s可以根据CPU、内存利用率或自定义指标，自动增加或减少Pod的副本数量，满足分布式系统的动态负载需求
    - 集群扩展CA（Cluster Autoscaler）：当集群资源不足时，K8s可以配合云厂商增加物理机器节点
4. 配置管理与环境一致性问题
    - configMap和Secret：将配置信息、密钥信息与镜像代码分离，修改配置后动态更新，避免重新构建镜像
    - 一致性环境：通过容器化技术，确保开发、测试、生产环境的一致性，减少环境差异带来的问题
5. 资源调度与利用率的问题
    - 调度器（Scheduler）：K8s调度器会根据各个节点的剩余资源、亲和性规则等因素，将Pod调度到最合适的节点上，提高资源利用率

K8s仍然无法解决的分布式系统的问题：

1. 数据一致性：K8s无法保证容器间的应用数据一致性，需要使用Paxos、Raft等分布式一致性算法胡总和分布式数据库来保证数据一致性
2. 分布式事务：K8s无法解决跨服务、跨节点的分布式事务问题，需要在应用层设计分布式事务解决方案
3. 熔断与细粒度限流：K8s有简单的流量控制，但是复杂的微服务治理（如熔断、细粒度限流）需要借助Istio等服务网格技术来实现

## 分布式理论

### 分布式系统的CAP理论

CAP理论是指在一个分布式系统不可能同时满足以下三个特性：

- **一致性（Consistency）**：指的强一致性，当向系统写入一个数据后，随后所有读取操作都能读取到最新写入的值，即无论用户访问哪一个节点，看到的数据都是一样的
- **可用性（Availability）**：指系统提供的服务必须一直处于可用状态，即使有部分节点发生故障，每一个非故障节点收到请求，都必须在一个合理的时间内给出响应
- **分区容错性（Partition Tolerance）**：指当分布式系统遇到网络分区，即节点之间通信断开形成孤岛时，系统仍能够继续运行

一般在分布式系统中，分区容错性P是必须要保证的，因为网络故障是不可避免的，如果不保证P，那么系统在网络分区时就会完全不可用，因此通常面临的是**CP**和**AP**的选择：

- **CP系统**：优先保证一致性和分区容错性，为了保证数据的强一致性，当出现网络故障的时候，系统会牺牲可用性而拒绝提供服务
    - 适用于对数据准确性要求极高的场景，如金融交易、配置中心，典型的CP系统有ETCD、HBase、Zookeeper等
- **AP系统**：优先保证可用性和分区容错性，为了保证服务不中断，牺牲数据的强一致性，允许不同节点的数据暂时不一致
    - 适用于对实时性要求高、但允许数据有一定延迟的场景，如社交网络、缓存系统，典型的AP系统有Eureka、DynamoDB等

### 分布式系统的BASE理论

由于CAP理论中的一致性要求过于严格，很多分布式系统选择了更为宽松的BASE理论来设计系统：

- **基本可用（Basically Available）**：系统保证基本的可用性，允许部分节点不可用，但整体服务仍然可用
- **软状态（Soft state）**：系统的状态不要求实时一致，可以有中间状态
- **最终一致性（Eventual consistency）**：系统允许数据在一段时间内不一致，但最终会达到一致的状态

可以把BASE理论看作是对CAP理论中AP系统的一个深入演化和工程化落地

## 分布式一致性

### 一致性问题

分布式系统中的一致性问题是指如何让多个独立节点对某个状态达成共识，因为在单机中内存是共享的，而在分布式系统中每个节点都有自己的内存空间，节点之间需要通过网络通信来交换信息，这就带来了数据同步和一致性的问题，解决方案有：

1. 强一致性解决方案
   - 共识算法：Paxos、Raft等算法通过多数节点达成一致，保证强一致性
   - 分布式锁：使用Zookeeper、etcd等实现分布式锁，确保同一时间只有一个节点能修改数据
   - 分布式事务：使用两阶段提交（2PC）、三阶段提交（3PC）等协议，确保跨节点操作的一致性
2. 最终一致性解决方案
   - TCC（Try-Confirm-Cancel）：通过预留资源、确认和取消操作，最终达到一致状态
   - 可靠消息最终一致性：通过消息队列确保消息最终被处理，达到数据一致

### 一致性算法

Raft和Paxos是两种经典的分布式一致性算法，旨在实现多阶段状态机的高可靠一致性，核心目标都是保证分布式系统的数据一致性

**Raft协议**

- **选举机制**：系统中的节点分为三种角色：领导者（Leader）、跟随者（Follower）和候选者（Candidate），系统启动时，所有阶段都是Follower，Follower会定期从Leader接收心跳信号以保持连接，如果Follower在一定时间内没有收到Leader的心跳信号，就会变成Candidate并发起选举，当Candidate获得超过半数节点的投票后，就会成为新的Leader，每个领导周期为一个任期（Term）
- **日志复制机制**：客户端请求会被Leader作为日志条目追加到自己的日志中，然后Leader会将这些日志条目复制到所有的跟Follower节点上，当大多数节点确认日志条目已经被复制后，Leader会将这条日志标记为已提交（Commit），并应用到状态机中，然后通知Follower应用这些已提交的日志

**Paxos协议**

- 系统中的节点分为三种角色：
  - 提议者（Proposer）：提出一致性问题的节点，向Accepter发送提议
  - 接受者（Acceptor）：负责处理提议并决定是否接受
  - 学习者（Learner）：负责学习最终达成一致的值
- **准备阶段**：提议者选择一个唯一的提议编号，并向所有接受者发送准备请求，接受者收到请求后，如果提议编号大于它之前见过的所有提议编号，就会回复一个承诺，表示不会接受编号小于该提议编号的任何提议
- **提议阶段**：提议者在收到大多数接受者的承诺后，选择一个值（可以是自己提出的值，也可以是接受者之前接受的值），并向所有接受者发送提议请求，接受者收到提议请求后，如果提议编号与之前承诺的编号一致，就会接受该提议并回复一个接受确认
- **学习阶段**：一旦有大多数接受者接受了某个提议，该提议被认为是达成共识的值，学习者会从接受者那里学习到这个值

对比总结：

1. Raft更易于理解和实现，将共识过程分解为选举和日志复制两个独立的部分，并且对选举超时时间等参数进行了明确的定义和限制，降低了算法的复杂度
2. Paxos是一种更通用、更基础的共识算法，理论性更强，但是实现相对复杂，理解和调试难度大

### 分布式锁

分布式锁是在分布式环境下来保证同一时刻，只有一个进程或节点能够访问共享资源的机制，核心目标是解决跨进程的资源竞争，分布式锁需要满足以下特性：

1. 互斥性：同一时间只能有一个客户端持有锁
2. 死锁避免：如果持有锁的客户端崩溃或网络断，锁能够被释放
3. 容错性：分布式锁服务本身需要高可用，不能成为单点故障
4. 只有加锁的客户端才能释放锁

分布式锁的实现方式：

- [Redis](./redis.md#分布式锁)
  - Redis是可以被多个客户端共享访问的单线程模型，并且Redis的读写性能很高，可以应对高并发的场景
  - `SET NX EX`命令中`NX`用于在key不存在时设置值，否则返回空，`EX`用于设置key的过期时间防止锁无法释放，结合使用可以实现简单的分布式锁
  - 加锁和解锁的操作需要使用Lua脚本保证原子性，防止误删他人锁

    ```vbnet
    SET lock_key unique_value NX EX 10  // 尝试加锁，10秒后过期
    // 解锁
    if GET lock_key == unique_value then
        DEL lock_key
    end if
    ```

- Zookeeper
  - 通过创建临时顺序节点来实现分布式锁，客户端创建一个临时顺序节点，所有客户端按节点名排序，最小的节点获得锁，当持有锁的客户端断开连接时，临时节点会被自动删除，释放锁
- etcd
  - 使用etcd的租约机制实现分布式锁，客户端创建一个带有租约的key，租约到期后自动删除key，释放锁
- 数据库
  - 通过数据库的行锁或表锁实现分布式锁，例如使用MySQL的`SELECT ... FOR UPDATE`语句来锁定某行数据

### 分布式事务

分布式系统中一个业务操作会由多个位于不同节点或不同服务的子操作组成，如何保证这些子操作要么全部成功，要么全部失败，从而保持数据的一致性，是分布式事务需要解决的问题，常见的解决方案有：

强一致性方案：

- 2PC（Two-Phase Commit，两阶段提交）
  - 准备阶段：协调者询问所有参与者是否准备好
  - 提交阶段：如果所有参与者都准备好，协调者发送提交命令，否则发送回滚命令
- 3PC（三阶段提交）
  - 引入超时机制和预提交阶段，减少阻塞时间，提高容错性

最终一致性方案：

- TCC（Try-Confirm-Cancel）
  - Try：预留资源
  - Confirm：确认操作，正式提交
  - Cancel：取消操作，业务回滚
- 可靠消息最终一致性
  - 使用消息队列确保消息最终被处理，达到数据一致

## 分布式组件

### 远程过程调用RPC

RPC允许程序调用运行在另一台计算机上的程序中的过程或函数，就像调用本地程序中的过程和函数一样，RPC隐藏了网络通信的复杂性，使得分布式系统的开发更加简单和高效

常见的RPC框架有：

- gRPC：由Google开发，基于HTTP/2和Protocol Buffers，支持多种编程语言
- Thrift：由Facebook开发，支持多种编程语言和数据格式
- Dubbo：阿里巴巴开源的RPC框架，主要用于Java生态

RPC和RESTful API的区别：

| 维度 | RPC | REST |
|------|-----|------|
| 设计核心 | 面向动作，强调调用远程方法 | 面向资源，强调操作资源的状态 |
| 协议 | 通常基于TCP或HTTP/2 | 通常使用HTTP/1.1协议 |
| 数据格式 | 常用二进制格式（如Protocol Buffers），体积小速度快 | 通常使用JSON或XML等文本格式，可读性好体积大 |
| 耦合度 | 耦合度较高，客户端需要了解服务端的方法签名 | 耦合度较低，客户端只需了解资源的URI和HTTP方法 |
| 性能 | 通常性能更高，序列化快，网络开销小 | HTTP Header较重，JSON解析耗时，性能相对较低 |

## 分布式场景问题

### 常见的限流算法

- 滑动窗口限流算法：
  - 将时间划分为多个小窗口，统计每个小窗口内的请求数，整体请求数为多个小窗口请求数之和，限定滑动窗口的请求总数不超过阈值
  - 优点：平滑限流，适应突发流量
  - 缺点：实现复杂，内存开销较大
- 令牌桶算法：
  - 系统以固定速率向桶中放入令牌（Token），请求到来时需要获取令牌才能继续处理
  - 优点：允许突发流量，实现简单
  - 缺点：可能会出现令牌耗尽的情况
- 漏桶算法：
  - 请求进入漏桶，漏桶以固定速率处理请求，超过容量的多余请求会被丢弃
  - 优点：平滑处理请求，防止突发流量
  - 缺点：不允许突发流量，可能会丢弃请求
